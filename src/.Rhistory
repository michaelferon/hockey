install.packages("ggplot2")
install.packages("MASS")
install.packages(c("fastDummies", "glmnet", "knitr", "leaps", "tidyverse"))
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA, warning = FALSE, message = FALSE, fig.align = "center")
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_398/Lectures/Lecture14")
library(lubridate)
library(ggplot2)
library(viridis)
library(cowplot)
vir.colors<-plasma(21)
EMLData <- read.csv("EML_through_09_12_2019.csv",
header=TRUE)
convertDate<- as.POSIXlt(EMLData$Date.Time,format="%m/%d/%y %H:%M")
EMLData$Date.Time<- convertDate
dates<-c("4/25", "5/08", "5/22", "6/05", "6/19", "7/03", "7/17", "7/31", "8/14", "8/28", "9/11")
date.loc<-c(115, 128, 142, 156, 170, 184, 198, 212, 226, 240, 254)-114
EMLData$yDay <- yday(EMLData$Date.Time)
maxDOsat <- tapply(EMLData$DOsat, yday(EMLData$Date.Time), max)
MaxDOsatFrame <- data.frame(max = maxDOsat, daysOfYear = seq(1:141))
g <- ggplot(data = MaxDOsatFrame, aes(x=daysOfYear, y=max) )
g + geom_point(aes(
color = cut_number(max,3, labels = c("Danger", "Warning", "Good")))) +
scale_x_continuous(name = "Dates", breaks = date.loc, labels = dates) +
labs(color="Levels", title="Maximum DOsat for Each Day") +
scale_color_manual(values=c("#DA5526", "#9C9EB5", "#2A385B")) +
geom_hline(yintercept=126, linetype="dashed",
color = "#DA5526", size=1) +
geom_hline(yintercept=159, linetype="dashed",
color = "#9C9EB5", size=1)
EMLData <- na.omit(EMLData)
EMLData$MonthObs <- month(EMLData$Date.Time)
EMLData$ProfileTime <- day(EMLData$Date.Time) + hour(EMLData$Date.Time)/24
MayData <- subset(EMLData, month(EMLData$Date.Time)==5)
meanMay <- tapply(MayData$Temp, MayData$Depth, mean)
maxMayInfo <- data.frame(
"meanDepth" = meanMay,
"Depth" = unique(MayData$Depth)
)
g1 <- ggplot(data = MayData, aes(x=Temp))
g1 <- g1 + geom_path(aes(y=Depth,alpha=0.95,group=ProfileTime,color=factor(ProfileTime)),size=0.15) +
scale_x_continuous(breaks=c(20,25,30),lim=c(min(EMLData$Temp),max(EMLData$Temp))) +
scale_color_manual(values = plasma(372))+
theme(
panel.background = element_blank(),
panel.border = element_rect(colour = "black", fill=NA, size=0.25),
legend.position = "none",
plot.title = element_text(size=7,face="bold",hjust=0.5),
axis.title.x = element_text(size=4),
axis.text.x = element_text(size=4),
axis.title.y = element_text(size=4),
axis.text.y = element_text(size=4),
axis.ticks = element_line(size=0.25)
) +
geom_path(data = maxMayInfo, aes(x = meanDepth, y=Depth)) +
scale_y_reverse(breaks=c(10,8,6,4,2,0)) +
labs(
title = "Temperature Profiles in May",
x = "Mean Temp (C)",
y="Depth (m)"
)
AugustData <- subset(EMLData, month(EMLData$Date.Time)==8)
meanAugust <- tapply(AugustData$Temp, AugustData$Depth, mean)
maxAugustInfo <- data.frame(
"meanDepth" = meanAugust,
"Depth" = unique(AugustData$Depth)
)
g2 <- ggplot(data = AugustData, aes(x=Temp))
g2 <- g2 + geom_path(aes(y=Depth,alpha=0.95,group=ProfileTime,color=factor(ProfileTime)),size=0.15) +
scale_x_continuous(breaks=c(20,25,30),lim=c(min(EMLData$Temp),max(EMLData$Temp))) +
scale_color_manual(values = plasma(372))+
theme(
panel.background = element_blank(),
panel.border = element_rect(colour = "black", fill=NA, size=0.25),
legend.position = "none",
plot.title = element_text(size=7,face="bold",hjust=0.5),
axis.title.x = element_text(size=4),
axis.text.x = element_text(size=4),
axis.title.y = element_text(size=4),
axis.text.y = element_text(size=4),
axis.ticks = element_line(size=0.25)
) +
geom_path(data = maxAugustInfo, aes(x = meanDepth, y=Depth)) +
scale_y_reverse(breaks=c(10,8,6,4,2,0)) +
labs(
title = "Temperature Profiles in August",
x = "Mean Temp (C)",
y="Depth (m)"
)
plot_grid(g1,g2, nrow=1)
averages <- as.matrix(with(mostYear, tapply(DOC, factor(Station_ID), mean),keepAttrs = TRUE))
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA, warning = FALSE, message = FALSE, fig.height = 7, fig.width = 11)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_398/Lectures/Lecture15")
library(lubridate)
library(fields)
library(viridis)
library(maps)
USGS <- read.csv(file="USGS_DOC.csv", header=TRUE, stringsAsFactors=TRUE)
USGS <- USGS[,-1]
convertDate<- as.POSIXlt(USGS$Date,format="%Y-%m-%d")
USGS$Date<- convertDate
uniqueStations <- factor(USGS$Station_ID)
print(length(levels(uniqueStations)))
yearObservations <- factor(year(USGS$Date))
print(table(yearObservations))
plot(yearObservations,xlab="Year",ylab="Observation Count",cex.lab=1.5)
mostYear <- subset(USGS, year(USGS$Date)==2002)
print(length(mostYear[,1]))
uniqueStations2 <- factor(mostYear$Station_ID)
print(length(levels(uniqueStations2)))
averages <- as.matrix(with(mostYear, tapply(DOC, factor(Station_ID), mean),keepAttrs = TRUE))
averages
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_437/hw6")
library(ggplot2)
concrete <- read.csv("concrete.csv", header=TRUE)
X <- concrete[,4:26]
y <- concrete$y
n = nrow(X)
W <- scale(X, scale=TRUE, center=TRUE)
gamma <- eigen(t(W)%*%W)$vectors
gamma2 <- eigen((n-1)*cor(X))$vectors
Z <- as.data.frame(as.matrix(W)%*%gamma)
num1 <- cbind(y,X)
num2 <- cbind(y,Z)
fit1 <- lm(y ~ ., data=num1)
fit2 <- lm(y ~ .-1, data=num2)
betaHat <- fit1$coefficients
gammaHat <- fit2$coefficients
g7 <- gammaHat
g7[8:23] <- 0
eig <- eigen(t(W)%*%W)
totVar <- sum(eig$values)
prop <- eig$values/totVar
cumProp <- cumsum(prop)
print(betaHat)
print(gammaHat)
print(cumProp)
print(g7)
ones <- rep(1,28)
yFull <- betaHat[1]*ones +  as.matrix(X)%*%betaHat[2:24]
xm = apply(X, 2, function(y) y - mean(y)) #center the columns of X
std = apply(X, 2, sd) #SDs of columns of X
s = diag(1/std)
w = xm%*%s
n = nrow(w)
gamma2 = eigen((n-1)*cor(w))$vectors
z = w%*%gamma2
ghat = lm(y~., data=num2)$coef
betastar = as.matrix(gamma2)%*%g7#Use only first k principal components,
#omit intercept
betahat = s%*%betastar
oneovern = array(1/n, c(1,n)) #row vector whose components are all 1/n
betahat0 = ones*(mean(y) - oneovern%*%as.matrix(X)%*%betahat)
yReduced <- betahat0 + as.matrix(X)%*%betahat
plot(yFull,yReduced)
abline(0,1)
View(num2)
View(num2)
install.packages("pls")
View(gamma)
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_437/hw6")
library(ggplot2)
concrete <- read.csv("concrete.csv", header=TRUE)
X <- concrete[,4:26]
y <- concrete$y
n = nrow(X)
W <- scale(X, scale=TRUE, center=TRUE)
gamma <- eigen(t(W)%*%W)$vectors
gamma2 <- eigen((n-1)*cor(X))$vectors
Z <- as.data.frame(as.matrix(W)%*%gamma)
num1 <- cbind(y,X)
num2 <- cbind(y,Z)
fit1 <- lm(y ~ ., data=num1)
fit2 <- lm(y ~ .-1, data=num2)
betaHat <- fit1$coefficients
gammaHat <- fit2$coefficients
g7 <- gammaHat
g7[8:23] <- 0
eig <- eigen(t(W)%*%W)
totVar <- sum(eig$values)
prop <- eig$values/totVar
cumProp <- cumsum(prop)
print(betaHat)
print(gammaHat)
print(cumProp)
print(g7)
ones <- rep(1,28)
yFull <- betaHat[1]*ones +  as.matrix(X)%*%betaHat[2:24]
xm = apply(X, 2, function(y) y - mean(y)) #center the columns of X
std = apply(X, 2, sd) #SDs of columns of X
s = diag(1/std)
w = xm%*%s
n = nrow(w)
gamma2 = eigen((n-1)*cor(w))$vectors
z = w%*%gamma2
ghat = lm(y~., data=num2)$coef
betastar = as.matrix(gamma2)%*%g7#Use only first k principal components,
#omit intercept
betahat = s%*%betastar
oneovern = array(1/n, c(1,n)) #row vector whose components are all 1/n
betahat0 = ones*(mean(y) - oneovern%*%as.matrix(X)%*%betahat)
yReduced <- betahat0 + as.matrix(X)%*%betahat
plot(yFull,yReduced)
abline(0,1)
set.seed(1)
library(pls)
pcr.fit <- pcr(y~., data=num2, validation="LOO")
summary(pcr.fit)
View(pcr.fit)
USGS <- read.csv(file="USGS_DOC.csv", header=TRUE, stringsAsFactors=FALSE)
library(lubridate)
library(viridis)
library(maps)
USGS <- read.csv(file="USGS_DOC.csv", header=TRUE, stringsAsFactors=FALSE)
USGS <- USGS[,-1]
convertDate<- as.POSIXlt(USGS$Date,format="%Y-%m-%d")
USGS$Date<- convertDate
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA, warning = FALSE, message = FALSE, fig.height = 7, fig.width = 11)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_398/Lectures/Lecture15")
library(lubridate)
library(fields)
\begin{titlepage}
\newgeometry{top=1in,bottom=1in,left=1in,right=1in,includefoot,includehead}
\begin{doublespace}
\centering
\includegraphics{logo}\\
{\LARGE Colorado School of Mines}
\vfill
\hrulefill\\[0.5cm]
{\Huge Precheck F-4}\\
\hrulefill
\vfill
{\Large Justin Nichols\\
Intro to Data Science with \textsf{R}---MATH 398\\
Professor Douglas Nychka\\[0.25cm]
Due: March \nth{4}, 2019}
\end{doublespace}
\end{titlepage}
\newpage
\restoregeometry
\pagestyle{fancy}
\benum
\item
How many unique Stations are there?
```{r}
uniqueStations <- factor(USGS$Station_ID)
print(length(levels(uniqueStations)))
```
\item
How many observations are there for each year? To answer this, you may need to know what are the first and last years in the dataset. Then, plot the number of ob- servations against the year. In which year were the highest number of observations collected?
```{r}
yearObservations <- factor(year(USGS$Date))
print(table(yearObservations))
plot(yearObservations,xlab="Year",ylab="Observation Count",cex.lab=1.5)
```
\eenum
The remaining questions focus just on the year with the highest number of observations.
```{r}
mostYear <- subset(USGS, year(USGS$Date)==2002)
```
\benumr
\item
How many observations are there in this year?
```{r}
print(length(mostYear[,1]))
```
\item
How many unique Station ID’s are there?
```{r}
uniqueStations2 <- factor(mostYear$Station_ID)
print(length(levels(uniqueStations2)))
```
\item
For each unique Station ID, average the corresponding DOC values to get a single DOC for each station. Show your code.
```{r, echo=TRUE}
averages <- as.matrix(with(mostYear, tapply(DOC, factor(Station_ID), mean),keepAttrs = TRUE))
```
\item
Plot the quilt plot of the average DOC values at each station location, choosing a color scheme from the \lstinline{viridis} package. Zoom out enough that you can overlap the US state boundaries.
\item
Find the latitude and longitude of Denver, and overlay this location with a single point in red. Choose a plotting character that makes this point different from the rest.
\item
What pattern, if any, do you observe in DOC in this year?
\item
Does it makes sense to apply a spatial smooth to this data? Explain your answer.
\eenum
\eenum
FALSE
FALSE
FALSE
FALSE
FALSE
FALSE
FALSE
FALSE
FALSE
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_398/Lectures/Lecture15")
library(lubridate)
library(fields)
library(viridis)
library(maps)
USGS <- read.csv(file="USGS_DOC.csv", header=TRUE, stringsAsFactors=FALSE)
USGS <- USGS[,-1]
convertDate<- as.POSIXlt(USGS$Date,format="%Y-%m-%d")
USGS$Date<- convertDate
uniqueStations <- factor(USGS$Station_ID)
print(length(levels(uniqueStations)))
yearObservations <- factor(year(USGS$Date))
print(table(yearObservations))
plot(yearObservations,xlab="Year",ylab="Observation Count",cex.lab=1.5)
mostYear <- subset(USGS, year(USGS$Date)==2002)
print(length(mostYear[,1]))
uniqueStations2 <- factor(mostYear$Station_ID)
print(length(levels(uniqueStations2)))
averages <- as.matrix(with(mostYear, tapply(DOC, factor(Station_ID), mean),keepAttrs = TRUE))
which(mostYear$Station_ID==rownames(averages))
rownames(averages)
class(rownames(averages))
mostYear$Station_ID[mostYear$Station_ID==rownames(averages)]
cut(mostYear$Station_ID)
factor(mostYear$Station_ID)
blah <- factor(mostYear$Station_ID)
averages <- as.matrix(with(mostYear, tapply(DOC, factor(Station_ID), mean),keepAttrs = TRUE))
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_437/hw6")
library(ggplot2)
concrete <- read.csv("concrete.csv", header=TRUE)
X <- concrete[,4:26]
y <- concrete$y
n = nrow(X)
W <- scale(X, scale=TRUE, center=TRUE)
gamma <- eigen(t(W)%*%W)$vectors
gamma2 <- eigen((n-1)*cor(X))$vectors
Z <- as.data.frame(as.matrix(W)%*%gamma)
num1 <- cbind(y,X)
num2 <- cbind(y,Z)
fit1 <- lm(y ~ ., data=num1)
fit2 <- lm(y ~ .-1, data=num2)
betaHat <- fit1$coefficients
gammaHat <- fit2$coefficients
g7 <- gammaHat
g7[8:23] <- 0
eig <- eigen(t(W)%*%W)
totVar <- sum(eig$values)
prop <- eig$values/totVar
cumProp <- cumsum(prop)
print(betaHat)
print(gammaHat)
print(cumProp)
print(g7)
ones <- rep(1,28)
yFull <- betaHat[1]*ones +  as.matrix(X)%*%betaHat[2:24]
xm = apply(X, 2, function(y) y - mean(y)) #center the columns of X
std = apply(X, 2, sd) #SDs of columns of X
s = diag(1/std)
w = xm%*%s
n = nrow(w)
gamma2 = eigen((n-1)*cor(w))$vectors
z = w%*%gamma2
ghat = lm(y~., data=num2)$coef
betastar = as.matrix(gamma2)%*%g7#Use only first k principal components,
#omit intercept
betahat = s%*%betastar
oneovern = array(1/n, c(1,n)) #row vector whose components are all 1/n
betahat0 = ones*(mean(y) - oneovern%*%as.matrix(X)%*%betahat)
yReduced <- betahat0 + as.matrix(X)%*%betahat
plot(yFull,yReduced)
abline(0,1)
set.seed(1)
library(pls)
pcr.fit <- pcr(y~., data=num2, validation="LOO")
summary(pcr.fit)
View(pcr.fit)
averages <- as.matrix(with(mostYear, tapply(DOC, factor(Station_ID), mean),keepAttrs = TRUE))
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA, warning = FALSE, message = FALSE, fig.height = 7, fig.width = 11)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_398/Lectures/Lecture15")
library(lubridate)
library(fields)
library(viridis)
library(maps)
USGS <- read.csv(file="USGS_DOC.csv", header=TRUE, stringsAsFactors=FALSE)
USGS <- USGS[,-1]
convertDate<- as.POSIXlt(USGS$Date,format="%Y-%m-%d")
USGS$Date<- convertDate
uniqueStations <- factor(USGS$Station_ID)
print(length(levels(uniqueStations)))
yearObservations <- factor(year(USGS$Date))
print(table(yearObservations))
plot(yearObservations,xlab="Year",ylab="Observation Count",cex.lab=1.5)
mostYear <- subset(USGS, year(USGS$Date)==2002)
print(length(mostYear[,1]))
uniqueStations2 <- factor(mostYear$Station_ID)
print(length(levels(uniqueStations2)))
averages <- as.matrix(with(mostYear, tapply(DOC, factor(Station_ID), mean),keepAttrs = TRUE))
View(mostYear)
View(averages)
View(mostYear)
View(averages)
averages <- tapply(mostYear$DOC, mostYear$Station_ID, mean)
names(averages)
quilt.plot(mostYear$Longitude, mostYear$Latitude, averages)
View(mostYear)
matches <- match(names(averages), mostYear$Station_ID)
matches
long <- mostYear$Longitude[matches]
long
lat <- mostYear$Latitude[matches]
quilt.plot(long,lat,averages)
quilt.plot(long,lat,averages, col=plasma(100))
quilt.plot(long,lat,averages, col=plasma(10))
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table,dvipsnames]{xcolor}', x, fixed = TRUE)})
knitr::opts_chunk$set(echo = FALSE,comment = NA, warning = FALSE, message = FALSE, fig.height = 7, fig.width = 11)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/Colorado School of Mines/Spring_2020/MATH_398/Lectures/Lecture15")
library(lubridate)
library(fields)
library(viridis)
library(maps)
USGS <- read.csv(file="USGS_DOC.csv", header=TRUE, stringsAsFactors=FALSE)
USGS <- USGS[,-1]
convertDate<- as.POSIXlt(USGS$Date,format="%Y-%m-%d")
USGS$Date<- convertDate
uniqueStations <- factor(USGS$Station_ID)
print(length(levels(uniqueStations)))
yearObservations <- factor(year(USGS$Date))
print(table(yearObservations))
plot(yearObservations,xlab="Year",ylab="Observation Count",cex.lab=1.5)
mostYear <- subset(USGS, year(USGS$Date)==2002)
print(length(mostYear[,1]))
uniqueStations2 <- factor(mostYear$Station_ID)
print(length(levels(uniqueStations2)))
averages <- tapply(mostYear$DOC, mostYear$Station_ID, mean)
matches <- match(names(averages), mostYear$Station_ID)
long <- mostYear$Longitude[matches]
lat <- mostYear$Latitude[matches]
quilt.plot(long,lat,averages, col=plasma(10))
US(add=TRUE)
quilt.plot(long,lat,averages, col=plasma(10))
US(add=TRUE)
load("~/Desktop/hockey/data/data.Rdata")
setwd("~/Desktop/hockey/src")
rm( list = ls() )
library(dplyr)
library(ggplot2)
library(lubridate)
# Load `df` and `X` data objects.
load('../data/data.Rdata')
X <- X %>% group_by(Season)
X.mean <- X %>% summarise_all(mean)
X.sd <- X %>% summarise_all(sd)
for (i in 1:10) {
m <- as.numeric(X.mean[i, -1])
s <- as.numeric(X.sd[i, -1])
# X[X$Season == i + 2008, -1] <- X[X$Season == i + 2008, -1]
}
rm( list = ls() )
library(dplyr)
library(ggplot2)
library(lubridate)
# Load `df` and `X` data objects.
load('../data/data.Rdata')
head(df)
df %>% glimpse()
df %>% group_by(Season)
df %>% group_by(Season) %>% scale()
df %>% group_by(Season) %>% as.numeric %>% scale()
View(X)
X %>% group_by(Season) %>% select(-1)
X %>% group_by(Season)
X %>% group_by(Season) %>% select(2:113)
X %>% group_by(Season) %>% select(1)
X %>% group_by(Season) %>% select(-1)
X %>% group_by(Season) %>% select(-2)
X %>% group_by(Season) %>% select(2)
X %>% group_by(Season) %>% scale()
X %>% group_by(Season) %>% select(2:113) %>% scale()
X %>% group_by(Season) %>% select(3:5) %>% scale()
X %>% group_by(Season) %>% select(3:5)
scale(X %>% group_by(Season)[2:113])
rm( list = ls() )
library(dplyr)
library(ggplot2)
library(lubridate)
# Load `df` and `ds`.
load('../data/data.Rdata')
View(df)
View(ds)
ds %>% coun
ds %>% count()
rm( list = ls() )
library(dplyr)
library(ggplot2)
library(lubridate)
# Load `df` and `ds`.
load('../data/data.Rdata')
# X is all the quantitative data.
X <- df %>% select(-c(1:11))
model <- lm(P ~ ., data = X)
# X is all the quantitative data.
X <- df %>% select(-c(1:11))
model <- lm(P ~ ., data = X)
